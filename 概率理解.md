概率论只不过是把常识用数学公式表达了出来。--拉普拉斯
概率之我见：
盒子中有10个球，3个白的，7个黑的，那么随便一抓，抓出来是白的的概率是3/10.
概率的计算方式其实很主观也很简单：目的情况数量 / 总的情况数量。所以计算概率时 1、总情况数量是多少 2、目标情况数量是多少。

贝叶斯定理：
Thomas Bayes是英国数学家，1763年首先提出这个定理。

首先理解下条件概率(conditional probability)：事件B发生的情况下，事件A发生的概率。用P(A|B)表示。
任何概率都是条件概率。--在全局条件下的概率，概率的问题用文氏图来看比较直观，如下：
![条件概率](image/p-1.jpg)
方框内为全局，黄色为A，蓝色变为B。那么在B发生的情况下，发生A的概率：总情况数为P(B)，A的概率是P(A∩B)，那么P(A|B)= P(AnB) / P(B).
除号，在分子大时可理解为，分子被分母平分。在分子小时可理解为，分子占分母的多少。
由上面公式得出：
p(AnB) = p(A|B)p(B)  同理=>
p(AnB) = p(B|A)p(A)  =>
p(A|B)p(B) = p(B|A)p(A) =>
p(A|B) = p(B|A)p(A) / p(B)
由此可见：条件概率公式也是从一个直观得出的公式中推到出的。

全概率公式：
如图：
![全概率](image/p-2.jpg)
假定样本空间为S，是两个时间A和A'的和，添加事件B，事件B也将样本划分为两个部分。
P(B) = P(BnA) + P(BnA')，
有条件概率公式可知：
P(BnA) = P(B|A)P(A),可以得出：
P(B) = P(B|A)P(A) + P(B|A')P(A')
这个就是全概率公式，全概率公式的含义是：如果A和A'构成样本空间的一个划分，那么事件B的概率，等于A和A'的概率分别乘以B对这两个事件的条件概率的和。

Tips:我们发现：有时我们通过主观得出一个公式，后来推到出另一个公式，用可以在这个公式中指导我们的认知。

贝叶斯推断的含义：
对条件概率公式进行变形-->
P(A|B) = P(A)(P(B|A) / P(B))，P(A)称为先验概率(Prior probability),即在B事件发生之前，对A事件概率的一个判断。P(A|B)称为后验概率，即在B事件发生之后，我们对A事件概率重新评估。P(B|A)/P(B)称为可能性函数(Likelyhood)，这是调整因子。所以，条件概率可以这么理解：
后验概率 = 先验概率 * 调整因子
贝叶斯推断：先预估一个先验概率，加入实验结果，看实验是增强还是削弱先验概率，由此得到更接近事实的后验概率。

一个栗子：
1号碗有30颗水果糖和10颗巧克力糖，2号碗水果糖和巧克力糖各20颗，现在随机选一个碗，从中拿出一颗糖，发现是水果糖，问这颗糖来自1号碗的概率？
设随意取一颗糖，来自1号碗的概率为P(H1)，由于1和2碗没有差，所以P(H1) = P(H2) = 0.5
设随意取一颗糖，是水果糖的概率是P(E1)，直观可得：(30 + 20) / 80 = 5/8
所求是E1已经发生的情况下，H1的概率即：
P(H1|E1) = P(E1|H1) P(H1) / P(E1) = 0.6
可见，取出水果糖之后H1事件的概率得到了增强，也说明事件E1的发生对于事件H1的发生是有影响的。

贝叶斯方法被证明是非常通用且强大的推理框架，深入了解下应用。
贝叶斯方法源于他解决一个【逆概】的问题。人们用常识都可以解决正向概率问题，譬如袋子有N个白球，M个黑球，摸出黑球的概率有多大。这个是个正向概率问题。如果把问题反过来，譬如：我们事先不知道袋子里黑白球的比例，然后我们取出一个或多个球，观察这些球的颜色，推测袋子中黑白球的比例。这个就是逆概率。
所有需要作出概率预测的地方都可以见到贝叶斯方法，贝叶斯是机器学习的核心方法之一。深刻原因是：由于人类的观察能力有局限，所以现实世界看起来是不确定的。这时我们需要有一个猜测，但不是瞎猜，要计算每种猜测的概率大小。

1、自然语言的二义性：
The girl saw the boy with a telescope.这是句有二义性的话，一般会说：那个女孩拿望远镜看到了那个男孩。或者也可以说：那个女孩看到了拿着望远镜的男孩。但是平常我们会更可能认为前一种意思。为什么人们会有这种倾向？
2、拼写纠正：
当用户输入了一个不在字典中的单词，我们去猜测它实际想输入什么，即我们要求：
P(实际想输入的单词|实际输入的单词)这个概率，并找出是这个概率最大的单词。可以使用贝叶斯公式求
