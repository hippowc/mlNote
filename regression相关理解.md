回归是什么？
一个比较好的解释：对现有的数据进行猜测（寻找数学模型），猜测结果中带有参数，再根据现有结果获得最佳参数，回归也叫最佳拟合。可以理解成获取数据关系的一种手段。

回归根据模型的不同分类：线性回归，逻辑回归，分类回归等等，每种模型也都有不同计算参数的方法，如：最小二乘法，最大似然法，梯度上升、梯度下降等


从一个例子开始：
假设有一组房屋销售数据，有面积和价钱两个数据，我们可以把面积作为特征数据，而价钱看做分类。
123平--250w，150平--320w，87平--160w，102平--220w... 以面积为x轴，以价钱为y轴，做出图发现点的排列有点像一条直线。现在有一个新的面积，如何猜测这个面积的价格？
如果使用回归的方法来解决问题步骤是这样的：通过一波训练数据，求得一个估计函数，然后通过函数对新数据进行一个估计。其中求估计函数是最重要的，包含了选择函数模型（一般会根据数据的分布情况，选择一个已知的函数去拟合）以及求的模型的系数。

线性回归模型：
假定特征和结果满足线性关系。其实线性关系表达能力很强大，每个特征对结果的影响可以由其前面的参数体现，而且每个特征变量可以首先映射到一个函数，然后参与线性计算，这样就可以表达特征与结果之间的非线性关系。
如果房子有多个feature，面积，朝向等，那么价格的函数可以表示为：
h(x) = O0 + O1x1 + O2x2, O是系数，用以调整feature在每个分量的影响力。然后这个方程还有一种向量表示法，假设x0 = 1：hO(x) = OT X
最后需要一个机制评估系数O是否比较好，通过损失函数或者错误函数来描述，之后通过一些方法调整O是的在损失函数中表现最好，一般通过最小二乘法，梯度下降法等。

逻辑回归与分类：
一般回归不用再分类问题上，因为回归结果是连续的，而我们分类的类型一般是离散的。而且求解函数系数时，噪声影响很大(噪声指的是一些特例独行的点)，如果要进行分类，可以使用逻辑回归。
逻辑回归本质上是线性回归，只是在特征到结果的映射中加入了一层函数映射，即先把特征线性求和，然后使用函数g(z)作为假设函数来预测。因为g(z)可以将连续值映射到0和1上。
即两个函数组成 g(z) = 1 / (1 + e^-z) 和 z = OT X
LR回归用来分类0/1问题，即预测结果属于0或者1的二值分类问题。假设二值满足伯努利分布，也就是：
P(y = 1 | x;O) = hO(x)
P(y = 0 | x;O) = 1 - hO(x)
两式何在一起： P(y | x;O) = (hO(x))^y (1-hO(x))^(1-y)

NOTE:
1 为什么采用逻辑回归模型进行分类？
http://www.cnblogs.com/jerrylead/archive/2011/03/05/1971867.html，先不深究
2 什么是伯努利分布？
伯努利分布是一种离散分布，有两种可能的结果，1表示成功，出现的概率为p，0表示失败，出现的概率为 1 - p
3 最大似然估计与最小二乘法？
4 梯度上升和梯度下降
梯度指的是多元函数在某一点处的一阶偏导数向量。
偏导数：
一元函数中，导数指的是函数的变化率，对于二元函数也要研究变化率。
